{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы оптимизации\n",
    "\n",
    "Как было показано на лекции, большинство методов машинного обучения сводятся к поиску параметров, которые минимизируют ошибку на тренировочной выборке:\n",
    "$$\n",
    "\\min_{w} L(w; D)\n",
    "$$\n",
    "Здесь:\n",
    "* $D$ — размеченная обучающая выборка, $\\{x_i, y_i\\}_{i=1}^N$\n",
    "* $L$ — функция потерь\n",
    "* $w$ — настраиваемые веса алгоритма\n",
    "\n",
    "В более общем виде задачу можно записать так:\n",
    "$$\n",
    "\\min_{x} f(x)\n",
    "$$\n",
    "Здесь:\n",
    "* $x$ — вектор значений\n",
    "* $f$ — функция, принимающая вектор в качестве аргумента и выдающая числовое значение.\n",
    "\n",
    "На семинаре рассмотрим подробнее методы минимизации функции, которые рассматривались на лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "Для оптимизации возьмем простую функцию $f(x) = x^3 - 2x^2 + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x ** 3 - 2*x ** 2 + 2\n",
    "df = lambda x: 3 * x ** 2 - 4 * x # производная\n",
    "x = np.linspace(-1, 2.5, 1000)\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim([-1, 2.5])\n",
    "plt.ylim([0, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И определим функцию, которая будет оптимизировать функцию $f(x)$ градиентным спуском с заданным постоянным шагом (он же learning rate, темп обучения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_and_plot_steps(learning_rate, x_new=2, compute_learning_rate=None):\n",
    "    x_old = 0\n",
    "    # x_new — точка старта\n",
    "    eps = 0.0001\n",
    "    x_list, y_list = [x_new], [f(x_new)] # инициализируем список координат и значений функций при итерации\n",
    "    \n",
    "    # спускаемся, пока разница между координатами не достигла требуемой точности\n",
    "    i = 0\n",
    "    while abs(x_new - x_old) > eps:\n",
    "        # обновляем значение темпа обучения, если нам задана функция для этого\n",
    "        if compute_learning_rate is not None:\n",
    "            learning_rate = compute_learning_rate(i, learning_rate)\n",
    "            \n",
    "        x_old = x_new\n",
    "        # считаем направление спуска\n",
    "        # делаем шаг\n",
    "        # Ваш код здесь!\n",
    "        \n",
    "        # запоминаем очередной шаг минимизации\n",
    "        x_list.append(x_new)\n",
    "        y_list.append(f(x_new))\n",
    "        i += 1\n",
    "        \n",
    "    print(\"Найденный локальный минимум:\", x_new)\n",
    "    print(\"Количество шагов:\", len(x_list))\n",
    "    \n",
    "    plt.figure(figsize=[10,3])\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(x_list, y_list, c=\"r\", edgecolors='k')\n",
    "    plt.plot(x_list, y_list, c=\"r\")\n",
    "    plt.plot(x, f(x), c=\"b\")\n",
    "    plt.xlim([-1,2.5])\n",
    "    plt.ylim([0,3])\n",
    "    plt.title(\"Descent trajectory\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(x_list,y_list,c=\"r\", edgecolors='k')\n",
    "    plt.plot(x_list,y_list,c=\"r\")\n",
    "    plt.plot(x,f(x), c=\"b\")\n",
    "    plt.xlim([1.2,2.1])\n",
    "    plt.ylim([0,3])\n",
    "    plt.title(\"Descent trajectory (zoomed in)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оптимизацию с шагом 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем шаг побольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что, если взять 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Застопорились в нуле, т.к. нашли точный локальный максимум. В нем производная равна нулю и мы никуда не можем сдвинуться. А если взять 0.49?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что, если взять 0.51?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы улетели далеко влево. Это можно понять, распечатав значения x_new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмём маленький шаг. Например, 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем меньше шаг, тем медленнее мы идём к минимум (и можем вдобавок застрять по пути). Чем больше темп обучения, тем большие расстояния мы перепрыгиваем (и имеем гипотетическую возможность найти минимум получше). Хорошая стратегия — начинать с достаточно большого шага (чтобы хорошо попутешествовать по функции), а потом постепенно его уменьшать, чтобы стабилизировать процесс обучения в каком-то локальном минимуме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем изменять шаг динамически:\n",
    "$lr(i + 1) = lr(i) * 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_learning_rate(i, prev_lr):\n",
    "    return prev_lr * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4, compute_learning_rate=compute_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сравнивать с постоянным темпом обучения, то мы нашли минимум в 2 раза быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это, конечно, искуственный пример, но такая же идея используются для обучения алгоритмов машинного обучения с миллионами параметров, функции потерь которых имеют очень сложную структуру и не поддаются визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка линейной регрессии с помощью градиентного спуска\n",
    "\n",
    "Рассмотрим теперь реальные данные и попробуем использовать градиентный спуск для решения задачи линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите файл food_trucks.txt. В нём два столбца значений — количество жителей в городе и доход грузовика с уличной едой в этом городе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируйте данные. По оси X — население города, по оси Y — доход грузовика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним функцию потерь линейной регрессии:\n",
    "$$\n",
    "L(w) = \\frac{1}{2m} \\sum_{i=1}^m (h(x^i, w) - y^i)^2\n",
    "$$\n",
    "Здесь $h(x, w) = w^Tx = w_0 + w_1 x_1$ (предполагается, что $x_0=1$ — дополнительный признак для удобства).\n",
    "$(x^i, y^i)$ — i-ый объект выборки.\n",
    "Тогда правило обновления весов будет выглядеть следующим образом:\n",
    "$$\n",
    "w_j = w_j - \\eta \\cdot \\frac{1}{m}\\sum_{i=1}^m(h(x^i, w) - y^i) x^i_j.\n",
    "$$\n",
    "Здесь $x^i_j$ — j-ая компонента i-ого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите функцию потерь и её производную. Эти функции имеют один аргумент — вектор весов $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию минимизации $L(w)$ с помощью градиентного спуска, аналогичную optimize_and_plot_steps. На вход она принимает параметры обучения (темп обучения и начальное значение весов), оптимизирует итеративно функцию потерь, печатает итерации и визуализирует уменьшение функции потерь и найденное решение. Запустите функцию с постоянным темпом обучения и прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измените функцию минимизации так, чтобы темп обучения мог меняться динамически, аналогично примеру выше. Запустите функцию и прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с батч-оптимизацией\n",
    "\n",
    "Теперь рассмотрим случай, когда данных в выборке много. В таких случаях используется стохастическая или батч-оптимизация. Первая состоит в том, что на каждом шаге итерации берется один объект, вторая — в том, что берется некоторое небольшое фиксированное количество объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные из файла space_ga.csv и нормализуйте их. Мы будем предсказывать первый столбец по шести остальным. Эти данные получены с выборов в США в 1980 году. Подробнее о столбцах можно прочитать тут: http://mldata.org/repository/data/viewslug/statlib-20050214-space_ga/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли заметить, датасет больше предыдущего. На нём мы попробуем батч-оптимизацию.\n",
    "\n",
    "Измените функцию минимизации так, чтобы на вход они принимала дополнительный параметр — размер батча. Для простоты проверки рекомендуется изменять копию функции, реализованной выше, с измененным именем. Запустите функцию при разных размерах батча. Прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
